apiVersion: v1
data:
  etl_iceberg.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.types import (
        StructType,
        StructField,
        IntegerType,
        StringType,
        DoubleType,
        TimestampType,
    )
    from faker import Faker
    import random
    from datetime import datetime, timedelta


    def generate_mock_sales_data(num_records=1000):
        """Generate realistic mock sales data using Faker"""
        fake = Faker()
        fake.seed_instance(42)

        data = []
        start_date = datetime(2025, 1, 1)

        for i in range(1, num_records + 1):
            product_categories = ["Electronics", "Clothing", "Books", "Home", "Sports"]
            product_category = random.choice(product_categories)

            if product_category == "Electronics":
                amount = round(random.uniform(50, 2000), 2)
            elif product_category == "Clothing":
                amount = round(random.uniform(20, 200), 2)
            elif product_category == "Books":
                amount = round(random.uniform(10, 50), 2)
            elif product_category == "Home":
                amount = round(random.uniform(30, 500), 2)
            else:
                amount = round(random.uniform(15, 300), 2)

            days_ago = random.randint(0, 365)

            data.append(
                (
                    i,
                    fake.name(),
                    fake.email(),
                    fake.city(),
                    fake.country(),
                    fake.catch_phrase(),
                    product_category,
                    amount,
                    start_date + timedelta(days=days_ago),
                    random.choice(["pending", "completed", "cancelled", "refunded"]),
                )
            )
        return data


    def main():
        # 1. VERSION CONFIGURATION FOR SPARK 4.x
        # Spark 4.1 uses Scala 2.13. We use the Iceberg 4.0 runtime (compatible with 4.x).
        ICEBERG_VERSION = "1.6.1"
        SPARK_MAJOR_VERSION = "4.0"
        SCALA_VERSION = "2.13"

        iceberg_pkg = f"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MAJOR_VERSION}_{SCALA_VERSION}:{ICEBERG_VERSION}"
        # Using a newer Hadoop-AWS jar compatible with Spark 4.x environments
        aws_pkg = "org.apache.hadoop:hadoop-aws:3.4.0"

        # 2. INITIALIZE SPARK SESSION
        spark = (
            SparkSession.builder.appName("Iceberg ETL Spark 4.1")
            .config("spark.jars.packages", f"{iceberg_pkg},{aws_pkg}")
            .config(
                "spark.sql.extensions",
                "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
            )
            # Define 'local' catalog using Hadoop for a local directory warehouse
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.local.type", "hadoop")
            .config("spark.sql.catalog.local.warehouse", "./iceberg_warehouse")
            .getOrCreate()
        )

        # Define schema
        schema = StructType(
            [
                StructField("order_id", IntegerType(), False),
                StructField("customer_name", StringType(), False),
                StructField("customer_email", StringType(), False),
                StructField("customer_city", StringType(), False),
                StructField("customer_country", StringType(), False),
                StructField("product_name", StringType(), False),
                StructField("product_category", StringType(), False),
                StructField("amount", DoubleType(), False),
                StructField("order_date", TimestampType(), False),
                StructField("order_status", StringType(), False),
            ]
        )

        print("--- Generating Data ---")
        mock_data = generate_mock_sales_data(1000)
        df = spark.createDataFrame(mock_data, schema)

        print("--- Writing to Iceberg ---")
        # We must use the 'local' catalog prefix defined in the config
        spark.sql("CREATE DATABASE IF NOT EXISTS local.demo")

        # Write the data
        df.writeTo("local.demo.sales").createOrReplace()

        print("--- Success! Table Details ---")
        # Verify by querying the table
        spark.sql("SELECT COUNT(*) as total_records FROM local.demo.sales").show()

        # Inspect Iceberg Metadata
        spark.sql("SELECT * FROM local.demo.sales.snapshots").show(truncate=False)

        spark.stop()


    if __name__ == "__main__":
        main()
kind: ConfigMap
metadata:
  labels:
    io.kompose.service: airflow
  name: airflow-cm1
