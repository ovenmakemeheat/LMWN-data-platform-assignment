{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg ETL Notebook\n",
    "\n",
    "This notebook demonstrates an ETL pipeline using Apache Spark and Iceberg tables.\n",
    "\n",
    "## Overview\n",
    "1. Initialize Spark Session with Iceberg configuration\n",
    "2. Generate mock sales data\n",
    "3. Write data to Iceberg table\n",
    "4. Query and analyze the data\n",
    "5. Explore Iceberg table features (snapshots, time travel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session\n",
    "\n",
    "The Spark session is pre-configured with Iceberg catalog settings via `spark-defaults.conf`:\n",
    "- Catalog: `demo` (Hive-based Iceberg catalog)\n",
    "- Warehouse: `s3a://warehouse/` (MinIO)\n",
    "- Metastore: `thrift://hive-metastore:9083`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    ")\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "# Configuration is loaded from spark-defaults.conf\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg ETL Notebook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Iceberg catalog is configured\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Schema and Generate Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for sales data\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"customer_email\", StringType(), False),\n",
    "    StructField(\"customer_city\", StringType(), False),\n",
    "    StructField(\"customer_country\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"order_date\", TimestampType(), False),\n",
    "    StructField(\"order_status\", StringType(), False),\n",
    "])\n",
    "\n",
    "print(\"Schema defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_sales_data(num_records=1000):\n",
    "    \"\"\"Generate realistic mock sales data using Faker\"\"\"\n",
    "    fake = Faker()\n",
    "    fake.seed_instance(42)\n",
    "\n",
    "    data = []\n",
    "    start_date = datetime(2025, 1, 1)\n",
    "\n",
    "    for i in range(1, num_records + 1):\n",
    "        product_categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home\", \"Sports\"]\n",
    "        product_category = random.choice(product_categories)\n",
    "\n",
    "        # Category-based pricing\n",
    "        if product_category == \"Electronics\":\n",
    "            amount = round(random.uniform(50, 2000), 2)\n",
    "        elif product_category == \"Clothing\":\n",
    "            amount = round(random.uniform(20, 200), 2)\n",
    "        elif product_category == \"Books\":\n",
    "            amount = round(random.uniform(10, 50), 2)\n",
    "        elif product_category == \"Home\":\n",
    "            amount = round(random.uniform(30, 500), 2)\n",
    "        else:\n",
    "            amount = round(random.uniform(15, 300), 2)\n",
    "\n",
    "        days_ago = random.randint(0, 365)\n",
    "\n",
    "        data.append((\n",
    "            i,\n",
    "            fake.name(),\n",
    "            fake.email(),\n",
    "            fake.city(),\n",
    "            fake.country(),\n",
    "            fake.catch_phrase(),\n",
    "            product_category,\n",
    "            amount,\n",
    "            start_date + timedelta(days=days_ago),\n",
    "            random.choice([\"pending\", \"completed\", \"cancelled\", \"refunded\"]),\n",
    "        ))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock data\n",
    "print(\"--- Generating Mock Sales Data ---\")\n",
    "mock_data = generate_mock_sales_data(1000)\n",
    "df = spark.createDataFrame(mock_data, schema)\n",
    "\n",
    "print(f\"Generated {df.count()} records\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write Data to Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database if not exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo.sales_db\")\n",
    "spark.sql(\"SHOW DATABASES IN demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to Iceberg table\n",
    "print(\"--- Writing to Iceberg Table ---\")\n",
    "df.writeTo(\"demo.sales_db.sales\").createOrReplace()\n",
    "\n",
    "print(\"Data written successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query and Analyze the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "print(\"--- Total Records ---\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_records FROM demo.sales_db.sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by category\n",
    "print(\"--- Sales by Category ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_category,\n",
    "        COUNT(*) as num_orders,\n",
    "        ROUND(SUM(amount), 2) as total_revenue,\n",
    "        ROUND(AVG(amount), 2) as avg_order_value\n",
    "    FROM demo.sales_db.sales\n",
    "    GROUP BY product_category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by status\n",
    "print(\"--- Sales by Status ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_status,\n",
    "        COUNT(*) as num_orders,\n",
    "        ROUND(SUM(amount), 2) as total_amount\n",
    "    FROM demo.sales_db.sales\n",
    "    GROUP BY order_status\n",
    "    ORDER BY num_orders DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 countries by revenue\n",
    "print(\"--- Top 10 Countries by Revenue ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_country,\n",
    "        COUNT(*) as num_orders,\n",
    "        ROUND(SUM(amount), 2) as total_revenue\n",
    "    FROM demo.sales_db.sales\n",
    "    GROUP BY customer_country\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Iceberg Table Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Iceberg snapshots\n",
    "print(\"--- Iceberg Snapshots ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history\n",
    "print(\"--- Table History ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table partitions\n",
    "print(\"--- Table Partitions ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table manifest files\n",
    "print(\"--- Manifest Files ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.manifests\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Operations (Optional)\n",
    "\n",
    "### Append More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional data and append\n",
    "print(\"--- Appending Additional Data ---\")\n",
    "additional_data = generate_mock_sales_data(500)\n",
    "# Adjust order_id to avoid duplicates\n",
    "additional_data = [(i + 1000, *rest) for i, *rest in additional_data]\n",
    "df_additional = spark.createDataFrame(additional_data, schema)\n",
    "\n",
    "df_additional.writeTo(\"demo.sales_db.sales\").append()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) as total_records FROM demo.sales_db.sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check snapshots after append\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first snapshot ID for time travel\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM demo.sales_db.sales.snapshots ORDER BY committed_at\").collect()\n",
    "if len(snapshots) > 1:\n",
    "    first_snapshot_id = snapshots[0][\"snapshot_id\"]\n",
    "    print(f\"--- Querying data at snapshot {first_snapshot_id} (before append) ---\")\n",
    "    spark.sql(f\"SELECT COUNT(*) as records_at_first_snapshot FROM demo.sales_db.sales VERSION AS OF {first_snapshot_id}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
