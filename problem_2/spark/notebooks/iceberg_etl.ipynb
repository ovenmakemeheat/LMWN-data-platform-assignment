{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg ETL Notebook\n",
    "\n",
    "This notebook demonstrates an ETL pipeline using Apache Spark and Iceberg tables.\n",
    "\n",
    "## Overview\n",
    "1. Initialize Spark Session with Iceberg configuration\n",
    "2. Generate mock sales data\n",
    "3. Write data to Iceberg table\n",
    "4. Query and analyze the data\n",
    "5. Explore Iceberg table features (snapshots, time travel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session\n",
    "\n",
    "The Spark session is pre-configured with Iceberg catalog settings via `spark-defaults.conf`:\n",
    "- Catalog: `demo` (Hive-based Iceberg catalog)\n",
    "- Warehouse: `s3a://warehouse/` (MinIO)\n",
    "- Metastore: `thrift://hive-metastore:9083`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-40.4.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-40.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org.apache.hadoop.fs.s3a.S3AFileSystem'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jsc.hadoopConfiguration().get(\"fs.s3a.impl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    ")\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 07:29:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://55c7962ddd4a:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70e5b9863a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark session\n",
    "# Configuration is loaded from spark-defaults.conf\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg ETL Notebook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|        local|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Iceberg catalog is configured\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Schema and Generate Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define schema for sales data\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"customer_email\", StringType(), False),\n",
    "    StructField(\"customer_city\", StringType(), False),\n",
    "    StructField(\"customer_country\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"order_date\", TimestampType(), False),\n",
    "    StructField(\"order_status\", StringType(), False),\n",
    "])\n",
    "\n",
    "print(\"Schema defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_sales_data(num_records=1000):\n",
    "    \"\"\"Generate realistic mock sales data using Faker\"\"\"\n",
    "    fake = Faker()\n",
    "    fake.seed_instance(42)\n",
    "\n",
    "    data = []\n",
    "    start_date = datetime(2025, 1, 1)\n",
    "\n",
    "    for i in range(1, num_records + 1):\n",
    "        product_categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home\", \"Sports\"]\n",
    "        product_category = random.choice(product_categories)\n",
    "\n",
    "        # Category-based pricing\n",
    "        if product_category == \"Electronics\":\n",
    "            amount = round(random.uniform(50, 2000), 2)\n",
    "        elif product_category == \"Clothing\":\n",
    "            amount = round(random.uniform(20, 200), 2)\n",
    "        elif product_category == \"Books\":\n",
    "            amount = round(random.uniform(10, 50), 2)\n",
    "        elif product_category == \"Home\":\n",
    "            amount = round(random.uniform(30, 500), 2)\n",
    "        else:\n",
    "            amount = round(random.uniform(15, 300), 2)\n",
    "\n",
    "        days_ago = random.randint(0, 365)\n",
    "\n",
    "        data.append((\n",
    "            i,\n",
    "            fake.name(),\n",
    "            fake.email(),\n",
    "            fake.city(),\n",
    "            fake.country(),\n",
    "            fake.catch_phrase(),\n",
    "            product_category,\n",
    "            amount,\n",
    "            start_date + timedelta(days=days_ago),\n",
    "            random.choice([\"pending\", \"completed\", \"cancelled\", \"refunded\"]),\n",
    "        ))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Mock Sales Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 records\n",
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- customer_name: string (nullable = false)\n",
      " |-- customer_email: string (nullable = false)\n",
      " |-- customer_city: string (nullable = false)\n",
      " |-- customer_country: string (nullable = false)\n",
      " |-- product_name: string (nullable = false)\n",
      " |-- product_category: string (nullable = false)\n",
      " |-- amount: double (nullable = false)\n",
      " |-- order_date: timestamp (nullable = false)\n",
      " |-- order_status: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Generate mock data\n",
    "print(\"--- Generating Mock Sales Data ---\")\n",
    "mock_data = generate_mock_sales_data(1000)\n",
    "df = spark.createDataFrame(mock_data, schema)\n",
    "\n",
    "print(f\"Generated {df.count()} records\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write Data to Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|demo.sales_db|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create database if not exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo.sales_db\")\n",
    "spark.sql(\"SHOW DATABASES IN demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Writing to Iceberg Table ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written successfully!\n"
     ]
    }
   ],
   "source": [
    "# Write data to Iceberg table\n",
    "print(\"--- Writing to Iceberg Table ---\")\n",
    "df.writeTo(\"demo.sales_db.sales\").createOrReplace()\n",
    "\n",
    "print(\"Data written successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query and Analyze the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total Records ---\n",
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|         1000|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the data\n",
    "print(\"--- Total Records ---\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_records FROM demo.sales_db.sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sales by Category ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-------------+---------------+\n",
      "|product_category|num_orders|total_revenue|avg_order_value|\n",
      "+----------------+----------+-------------+---------------+\n",
      "|     Electronics|       205|    217276.27|        1059.88|\n",
      "|            Home|       208|     53558.26|         257.49|\n",
      "|          Sports|       180|     27978.69|         155.44|\n",
      "|        Clothing|       203|     20396.55|         100.48|\n",
      "|           Books|       204|      6243.34|           30.6|\n",
      "+----------------+----------+-------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Sales by category\n",
    "print(\"--- Sales by Category ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_category,\n",
    "        COUNT(*) as num_orders,\n",
    "        ROUND(SUM(amount), 2) as total_revenue,\n",
    "        ROUND(AVG(amount), 2) as avg_order_value\n",
    "    FROM demo.sales_db.sales\n",
    "    GROUP BY product_category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sales by Status ---\n",
      "+------------+----------+------------+\n",
      "|order_status|num_orders|total_amount|\n",
      "+------------+----------+------------+\n",
      "|   cancelled|       264|    78721.21|\n",
      "|   completed|       257|     81878.5|\n",
      "|    refunded|       248|    84431.24|\n",
      "|     pending|       231|    80422.16|\n",
      "+------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sales by status\n",
    "print(\"--- Sales by Status ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_status,\n",
    "        COUNT(*) as num_orders,\n",
    "        ROUND(SUM(amount), 2) as total_amount\n",
    "    FROM demo.sales_db.sales\n",
    "    GROUP BY order_status\n",
    "    ORDER BY num_orders DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 10 Countries by Revenue ---\n",
      "+--------------------+----------+-------------+\n",
      "|    customer_country|num_orders|total_revenue|\n",
      "+--------------------+----------+-------------+\n",
      "|                Oman|         7|      5692.34|\n",
      "|       French Guiana|         7|      5576.95|\n",
      "|Saint Pierre and ...|         4|      5447.18|\n",
      "|Holy See (Vatican...|         4|      4748.24|\n",
      "|             Morocco|         9|      4722.49|\n",
      "|           Nicaragua|         5|      4335.34|\n",
      "|             Somalia|         5|      4267.92|\n",
      "|          Cape Verde|         8|      4080.69|\n",
      "|       Liechtenstein|         5|      4008.78|\n",
      "|             Burundi|         4|      3950.74|\n",
      "+--------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 countries by revenue\n",
    "print(\"--- Top 10 Countries by Revenue ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_country,\n",
    "        COUNT(*) as num_orders,\n",
    "        ROUND(SUM(amount), 2) as total_revenue\n",
    "    FROM demo.sales_db.sales\n",
    "    GROUP BY customer_country\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Iceberg Table Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iceberg Snapshots ---\n",
      "+-----------------------+-------------------+---------+---------+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id|operation|manifest_list                                                                                                   |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+-----------------------+-------------------+---------+---------+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2026-02-23 06:53:31.027|6664706160060697349|NULL     |append   |$PWD/warehouse/demo/sales_db/sales/metadata/snap-6664706160060697349-1-1f7817a4-2ac8-4737-a278-92aa286c9f38.avro|{spark.app.id -> local-1771829551481, added-data-files -> 16, added-records -> 1000, added-files-size -> 126604, changed-partition-count -> 1, total-records -> 1000, total-files-size -> 126604, total-data-files -> 16, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.5, app-id -> local-1771829551481, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.1 (commit 9ce0fcf0af7becf25ad9fc996c3bad2afdcfd33d)}|\n",
      "|2026-02-23 06:56:18.531|3438309255227403268|NULL     |append   |$PWD/warehouse/demo/sales_db/sales/metadata/snap-3438309255227403268-1-e56581c1-50be-402a-9ff6-b4a105fd6b92.avro|{spark.app.id -> local-1771829551481, added-data-files -> 16, added-records -> 1000, added-files-size -> 126604, changed-partition-count -> 1, total-records -> 1000, total-files-size -> 126604, total-data-files -> 16, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.5, app-id -> local-1771829551481, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.1 (commit 9ce0fcf0af7becf25ad9fc996c3bad2afdcfd33d)}|\n",
      "+-----------------------+-------------------+---------+---------+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View Iceberg snapshots\n",
    "print(\"--- Iceberg Snapshots ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Table History ---\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2026-02-23 06:53:...|6664706160060697349|               NULL|              false|\n",
      "|2026-02-23 06:56:...|3438309255227403268|               NULL|               true|\n",
      "|2026-02-23 06:56:...|8488913132314979929|3438309255227403268|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "print(\"--- Table History ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.history\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Table Partitions ---\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|        1500|        32|                       220841|                           0|                         0|                           0|                         0|2026-02-23 06:56:...|     8488913132314979929|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table partitions\n",
    "print(\"--- Table Partitions ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.partitions\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Manifest Files ---\n",
      "+-------+--------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|                path|length|partition_spec_id|  added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+--------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|      0|$PWD/warehouse/de...| 10049|                0|8488913132314979929|                    16|                        0|                       0|                       0|                          0|                         0|                 []|\n",
      "|      0|$PWD/warehouse/de...|  9937|                0|3438309255227403268|                    16|                        0|                       0|                       0|                          0|                         0|                 []|\n",
      "+-------+--------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table manifest files\n",
    "print(\"--- Manifest Files ---\")\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.manifests\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Operations (Optional)\n",
    "\n",
    "### Append More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Appending Additional Data ---\n",
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|         2000|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate additional data and append\n",
    "print(\"--- Appending Additional Data ---\")\n",
    "additional_data = generate_mock_sales_data(500)\n",
    "# Adjust order_id to avoid duplicates\n",
    "additional_data = [(i + 1000, *rest) for i, *rest in additional_data]\n",
    "df_additional = spark.createDataFrame(additional_data, schema)\n",
    "\n",
    "df_additional.writeTo(\"demo.sales_db.sales\").append()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) as total_records FROM demo.sales_db.sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2026-02-23 06:53:...|6664706160060697349|               NULL|   append|$PWD/warehouse/de...|{spark.app.id -> ...|\n",
      "|2026-02-23 06:56:...|3438309255227403268|               NULL|   append|$PWD/warehouse/de...|{spark.app.id -> ...|\n",
      "|2026-02-23 06:56:...|8488913132314979929|3438309255227403268|   append|$PWD/warehouse/de...|{spark.app.id -> ...|\n",
      "|2026-02-23 06:57:...| 479311417805473928|8488913132314979929|   append|$PWD/warehouse/de...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check snapshots after append\n",
    "spark.sql(\"SELECT * FROM demo.sales_db.sales.snapshots\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Querying data at snapshot 6664706160060697349 (before append) ---\n",
      "+-------------------------+\n",
      "|records_at_first_snapshot|\n",
      "+-------------------------+\n",
      "|                     1000|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first snapshot ID for time travel\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM demo.sales_db.sales.snapshots ORDER BY committed_at\").collect()\n",
    "if len(snapshots) > 1:\n",
    "    first_snapshot_id = snapshots[0][\"snapshot_id\"]\n",
    "    print(f\"--- Querying data at snapshot {first_snapshot_id} (before append) ---\")\n",
    "    spark.sql(f\"SELECT COUNT(*) as records_at_first_snapshot FROM demo.sales_db.sales VERSION AS OF {first_snapshot_id}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
