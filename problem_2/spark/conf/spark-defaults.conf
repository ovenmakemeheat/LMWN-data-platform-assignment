# spark.sql.extensions                            org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
# # ^ Enables Iceberg-specific SQL extensions (ALTER TABLE ..., VERSION AS OF ..., etc.). No Iceberg magic without it!

# spark.sql.catalog.hive_prod                     org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.hive_prod.type                hive
# spark.sql.catalog.hive_prod.uri                 thrift://hive-metastore:9083 # The address of my HMS
# spark.sql.catalog.hive_prod.warehouse           s3a://warehouse/hive_prod_db # Where data for tables in this catalog will be written on MinIO.
# # S3A Config for MinIO: so Spark can read/write to my local MinIO.
# spark.hadoop.fs.s3a.endpoint                    http://minio:9000
# spark.hadoop.fs.s3a.access.key                  admin
# spark.hadoop.fs.s3a.secret.key                  password
# spark.hadoop.fs.s3a.path.style.access           true
# spark.hadoop.fs.s3a.impl                        org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.connection.ssl.enabled      false # My MinIO runs on HTTP.s

# spark.jars.packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262

# spark.jars.packages                                  org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.1
# spark.sql.extensions                                 org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
# spark.sql.catalog.spark_catalog                      org.apache.iceberg.spark.SparkSessionCatalog
# spark.sql.catalog.spark_catalog.type                 hive
# spark.sql.catalog.local                              org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.local.type                         hadoop
# spark.sql.catalog.local.warehouse                    $PWD/warehouse
# spark.sql.defaultCatalog                             local

spark.metrics.conf /opt/spark/conf/metrics.properties
spark.ui.prometheus.enabled true